{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1caba7eaf10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.linspace(-2,2,100)\n",
    "y = x**2\n",
    "plt.plot(x,y)\n",
    "plt.scatter(0,0,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X：1.60, Y：4.00，gx：4.00\n",
      "X：1.28, Y：2.56，gx：3.20\n",
      "X：1.02, Y：1.64，gx：2.56\n",
      "X：0.82, Y：1.05，gx：2.05\n",
      "X：0.66, Y：0.67，gx：1.64\n",
      "X：0.52, Y：0.43，gx：1.31\n",
      "X：0.42, Y：0.27，gx：1.05\n",
      "X：0.34, Y：0.18，gx：0.84\n",
      "X：0.27, Y：0.11，gx：0.67\n",
      "X：0.21, Y：0.07，gx：0.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21474836480000006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def min_gradient(x_start, rate, n_iter, f,g):\n",
    "    x = x_start\n",
    "    for n in range(n_iter):\n",
    "        gx = g(x)\n",
    "        y = f(x)\n",
    "        x = x - rate*gx # 梯度下降\n",
    "        print(\"X:{x:.2f}, Y:{y:.2f}, gx:{gx:.2f}\".format(x=x, y=y,gx=gx))\n",
    "        if abs(gx)<0.0001:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "f = lambda x:x**2\n",
    "g = lambda x: 2*x\n",
    "min_gradient(x_start=2,rate=0.1, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X：0.80, Y：4.00，gx：4.00\n",
      "X：0.32, Y：0.64，gx：1.60\n",
      "X：0.13, Y：0.10，gx：0.64\n",
      "X：0.05, Y：0.02，gx：0.26\n",
      "X：0.02, Y：0.00，gx：0.10\n",
      "X：0.01, Y：0.00，gx：0.04\n",
      "X：0.00, Y：0.00，gx：0.02\n",
      "X：0.00, Y：0.00，gx：0.01\n",
      "X：0.00, Y：0.00，gx：0.00\n",
      "X：0.00, Y：0.00，gx：0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00020971520000000014"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_gradient(x_start=2,rate=0.3, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X：-2.40, Y：4.00，gx：4.00\n",
      "X：2.88, Y：5.76，gx：-4.80\n",
      "X：-3.46, Y：8.29，gx：5.76\n",
      "X：4.15, Y：11.94，gx：-6.91\n",
      "X：-4.98, Y：17.20，gx：8.29\n",
      "X：5.97, Y：24.77，gx：-9.95\n",
      "X：-7.17, Y：35.66，gx：11.94\n",
      "X：8.60, Y：51.36，gx：-14.33\n",
      "X：-10.32, Y：73.95，gx：17.20\n",
      "X：12.38, Y：106.49，gx：-20.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.383472844800014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_gradient(x_start=2,rate=1.1, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.ones(5, 3,dtype=torch.float64)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "y_array= np.random.randn(5,3)\n",
    "y_tensor = torch.from_numpy(y_array)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.add(x_tensor,y_tensor)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor.add_(y_tensor)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_tensor = torch.randn(1, 4)\n",
    "print(x_tensor)\n",
    "y_tensor = torch.randn(4, 1)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(x_tensor,y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tensor = x_tensor.view(-1, 8) \n",
    "z_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tensor.tolist()\n",
    "z_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True) # 定义一个值为3的tensor名字叫作x\n",
    "y = x*x   # 定义y和x的关系\n",
    "y.backward()  # 自动计算梯度\n",
    "print(x.grad) #打印y相对于x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x*x\n",
    "z.backward()  \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "z = x*x\n",
    "z.backward()  \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gred(x_start, rate, num, f):\n",
    "    x = torch.tensor(x_start, requires_grad=True)\n",
    "    for n in range(num): \n",
    "        y = f(x)\n",
    "        y.backward()   # 计算梯度\n",
    "        gx = x.grad.tolist()  # 取出梯度\n",
    "        newx = x.tolist() - rate*gx # 修正\n",
    "        x = torch.tensor(newx, requires_grad=True)  #重新定义x, 所以不需要清零操作\n",
    "        print(\"X:{x:.2f}, Y:{y:.2f},gx:{gx:.2f}\".format(x=x, y=y,gx=gx))\n",
    "        if abs(gx)<0.0001:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x**2\n",
    "min_gred(2.0, 0.1, 10,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[20,2])\n",
    "y = 2*X[:,0] + 2*X[:,1]\n",
    "y = y.reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Simple:\n",
    "\n",
    "    def __init__(self, dim,rate=0.1, n_iter=20):\n",
    "        self.rate = rate  # 步长，学习速率\n",
    "        self.n_iter = n_iter  # 迭代次数\n",
    "        self.W = np.random.randn(1, dim)  # 代表被训练的系数\n",
    "        self.MSE = []   # 用于保存损失的空list\n",
    "\n",
    "    def fit(self, X, y):  # 训练函数\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.predict(X)  # 计算预测的Y\n",
    "            errors = y - output  \n",
    "            g = np.dot(errors.T, X)\n",
    "            self.W += self.rate * g  # 根据更新规则更新系数\n",
    "            self.MSE.append((errors**2).sum())  # 记录损失函数的值\n",
    "            #print(self.W)\n",
    "\n",
    "\n",
    "    def predict(self, X):   # 给定系数和X计算预测的Y\n",
    "        output = np.dot(X, self.W.T)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.34394649 2.24996708]]\n",
      "[[2.05276898 1.89683745]]\n",
      "[[2.07460705 1.95105854]]\n",
      "[[2.05993297 1.95348239]]\n",
      "[[2.0515301  1.96094638]]\n",
      "[[2.0438626  1.96664231]]\n",
      "[[2.0373901  1.97157888]]\n",
      "[[2.03186603 1.97577611]]\n",
      "[[2.02715891 1.97935458]]\n",
      "[[2.02314701 1.98240428]]\n",
      "[[2.01972776 1.98500351]]\n",
      "[[2.01681359 1.98721877]]\n",
      "[[2.0143299 1.9891068]]\n",
      "[[2.0122131  1.99071594]]\n",
      "[[2.010409   1.99208737]]\n",
      "[[2.00887139 1.99325622]]\n",
      "[[2.00756092 1.9942524 ]]\n",
      "[[2.00644402 1.99510143]]\n",
      "[[2.00549212 1.99582504]]\n",
      "[[2.00468083 1.99644176]]\n",
      "[[2.00398938 1.99696738]]\n",
      "[[2.00340007 1.99741536]]\n",
      "[[2.00289782 1.99779716]]\n",
      "[[2.00246975 1.99812256]]\n",
      "[[2.00210493 1.99839989]]\n",
      "[[2.00179399 1.99863626]]\n",
      "[[2.00152898 1.99883771]]\n",
      "[[2.00130312 1.9990094 ]]\n",
      "[[2.00111063 1.99915573]]\n",
      "[[2.00094657 1.99928045]]\n"
     ]
    }
   ],
   "source": [
    "simple_nn = NeuralNet_Simple(dim = X.shape[1], rate=0.001, n_iter=30)\n",
    "simple_nn.fit(X, y);  # 喂入数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.98502448 2.01138397]]\n"
     ]
    }
   ],
   "source": [
    "print(simple_nn.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13814.357482525184,\n",
       " 753.5600852621352,\n",
       " 406.3194115985948,\n",
       " 292.99967460698474,\n",
       " 212.79734529553224,\n",
       " 154.5718381781489,\n",
       " 112.2783184324237,\n",
       " 81.55704018209796,\n",
       " 59.24163191311824,\n",
       " 43.03210297963435,\n",
       " 31.257779825624915,\n",
       " 22.70511390274518,\n",
       " 16.492604408007615,\n",
       " 11.979944312287497,\n",
       " 8.702025597353614,\n",
       " 6.321001794584999,\n",
       " 4.591467037203111,\n",
       " 3.335162722431545,\n",
       " 2.422604865714705,\n",
       " 1.7597385266724441,\n",
       " 1.278243813541494,\n",
       " 0.9284943314542868,\n",
       " 0.6744423203224585,\n",
       " 0.4899033069264648,\n",
       " 0.3558573400654087,\n",
       " 0.2584886541650412,\n",
       " 0.18776171462354532,\n",
       " 0.1363868816302611,\n",
       " 0.0990690861452904,\n",
       " 0.07196208104728015]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.00060934])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.predict([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[20,2])\n",
    "y_area = X[:,0]*X[:,1]\n",
    "y_area = y_area.reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = NeuralNet_Simple(dim = X.shape[1], rate=0.001, n_iter=20)\n",
    "simple_nn.fit(X, y);  # 喂入数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25252.149786162016,\n",
       " 1999.5347186589672,\n",
       " 1964.4150729643118,\n",
       " 1953.2435632290033,\n",
       " 1944.6993711647647,\n",
       " 1938.1539679667644,\n",
       " 1933.139756763213,\n",
       " 1929.2985397613165,\n",
       " 1926.3559138014748,\n",
       " 1924.101667937402,\n",
       " 1922.3747666984953,\n",
       " 1921.051846276874,\n",
       " 1920.0384018467496,\n",
       " 1919.2620364689374,\n",
       " 1918.6672893057482,\n",
       " 1918.2116736919784,\n",
       " 1917.8626420355713,\n",
       " 1917.5952607310417,\n",
       " 1917.3904289895659,\n",
       " 1917.233514345179]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.9940174])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.predict([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "n_iter = 30\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size, output_size, bias=False)\n",
    "# 定义损失函数和最优化方法\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 422.5421\n",
      "Epoch [2/30], Loss: 332.9817\n",
      "Epoch [3/30], Loss: 262.4106\n",
      "Epoch [4/30], Loss: 206.8023\n",
      "Epoch [5/30], Loss: 162.9841\n",
      "Epoch [6/30], Loss: 128.4561\n",
      "Epoch [7/30], Loss: 101.2486\n",
      "Epoch [8/30], Loss: 79.8091\n",
      "Epoch [9/30], Loss: 62.9149\n",
      "Epoch [10/30], Loss: 49.6020\n",
      "Epoch [11/30], Loss: 39.1111\n",
      "Epoch [12/30], Loss: 30.8440\n",
      "Epoch [13/30], Loss: 24.3290\n",
      "Epoch [14/30], Loss: 19.1947\n",
      "Epoch [15/30], Loss: 15.1484\n",
      "Epoch [16/30], Loss: 11.9593\n",
      "Epoch [17/30], Loss: 9.4458\n",
      "Epoch [18/30], Loss: 7.4646\n",
      "Epoch [19/30], Loss: 5.9028\n",
      "Epoch [20/30], Loss: 4.6716\n",
      "Epoch [21/30], Loss: 3.7009\n",
      "Epoch [22/30], Loss: 2.9354\n",
      "Epoch [23/30], Loss: 2.3316\n",
      "Epoch [24/30], Loss: 1.8554\n",
      "Epoch [25/30], Loss: 1.4795\n",
      "Epoch [26/30], Loss: 1.1829\n",
      "Epoch [27/30], Loss: 0.9487\n",
      "Epoch [28/30], Loss: 0.7636\n",
      "Epoch [29/30], Loss: 0.6173\n",
      "Epoch [30/30], Loss: 0.5016\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    # 数据转换\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y).to(torch.float)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[2.0354, 1.8766]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[100,2])\n",
    "y_area = X[:,0]*X[:,1]\n",
    "y_area= y_area.reshape(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 2\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "n_iter = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.activate = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activate(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1246.3212\n",
      "Epoch [2/1000], Loss: 1174.0906\n",
      "Epoch [3/1000], Loss: 1104.6456\n",
      "Epoch [4/1000], Loss: 1037.7800\n",
      "Epoch [5/1000], Loss: 973.2347\n",
      "Epoch [6/1000], Loss: 910.7509\n",
      "Epoch [7/1000], Loss: 850.0472\n",
      "Epoch [8/1000], Loss: 791.1007\n",
      "Epoch [9/1000], Loss: 733.8785\n",
      "Epoch [10/1000], Loss: 678.3557\n",
      "Epoch [11/1000], Loss: 624.5042\n",
      "Epoch [12/1000], Loss: 572.4677\n",
      "Epoch [13/1000], Loss: 522.3838\n",
      "Epoch [14/1000], Loss: 474.4445\n",
      "Epoch [15/1000], Loss: 428.8578\n",
      "Epoch [16/1000], Loss: 385.9288\n",
      "Epoch [17/1000], Loss: 346.0201\n",
      "Epoch [18/1000], Loss: 309.4921\n",
      "Epoch [19/1000], Loss: 276.5892\n",
      "Epoch [20/1000], Loss: 247.7615\n",
      "Epoch [21/1000], Loss: 223.3052\n",
      "Epoch [22/1000], Loss: 203.4137\n",
      "Epoch [23/1000], Loss: 188.2515\n",
      "Epoch [24/1000], Loss: 177.7931\n",
      "Epoch [25/1000], Loss: 171.7816\n",
      "Epoch [26/1000], Loss: 169.7733\n",
      "Epoch [27/1000], Loss: 171.1062\n",
      "Epoch [28/1000], Loss: 174.9105\n",
      "Epoch [29/1000], Loss: 180.2314\n",
      "Epoch [30/1000], Loss: 186.0621\n",
      "Epoch [31/1000], Loss: 191.4881\n",
      "Epoch [32/1000], Loss: 195.7766\n",
      "Epoch [33/1000], Loss: 198.4366\n",
      "Epoch [34/1000], Loss: 199.2270\n",
      "Epoch [35/1000], Loss: 198.1627\n",
      "Epoch [36/1000], Loss: 195.4764\n",
      "Epoch [37/1000], Loss: 191.5355\n",
      "Epoch [38/1000], Loss: 186.7713\n",
      "Epoch [39/1000], Loss: 181.5886\n",
      "Epoch [40/1000], Loss: 176.3373\n",
      "Epoch [41/1000], Loss: 171.3239\n",
      "Epoch [42/1000], Loss: 166.7608\n",
      "Epoch [43/1000], Loss: 162.9058\n",
      "Epoch [44/1000], Loss: 160.1445\n",
      "Epoch [45/1000], Loss: 158.2349\n",
      "Epoch [46/1000], Loss: 157.0637\n",
      "Epoch [47/1000], Loss: 156.4777\n",
      "Epoch [48/1000], Loss: 156.2933\n",
      "Epoch [49/1000], Loss: 156.3293\n",
      "Epoch [50/1000], Loss: 156.4698\n",
      "Epoch [51/1000], Loss: 156.5128\n",
      "Epoch [52/1000], Loss: 156.3529\n",
      "Epoch [53/1000], Loss: 155.9164\n",
      "Epoch [54/1000], Loss: 155.1758\n",
      "Epoch [55/1000], Loss: 154.2195\n",
      "Epoch [56/1000], Loss: 152.9429\n",
      "Epoch [57/1000], Loss: 151.4227\n",
      "Epoch [58/1000], Loss: 149.7435\n",
      "Epoch [59/1000], Loss: 147.9918\n",
      "Epoch [60/1000], Loss: 146.2509\n",
      "Epoch [61/1000], Loss: 144.5867\n",
      "Epoch [62/1000], Loss: 143.0949\n",
      "Epoch [63/1000], Loss: 142.0337\n",
      "Epoch [64/1000], Loss: 140.9630\n",
      "Epoch [65/1000], Loss: 139.7541\n",
      "Epoch [66/1000], Loss: 138.5299\n",
      "Epoch [67/1000], Loss: 137.4032\n",
      "Epoch [68/1000], Loss: 136.4127\n",
      "Epoch [69/1000], Loss: 135.3987\n",
      "Epoch [70/1000], Loss: 134.3479\n",
      "Epoch [71/1000], Loss: 133.2512\n",
      "Epoch [72/1000], Loss: 132.1077\n",
      "Epoch [73/1000], Loss: 130.9221\n",
      "Epoch [74/1000], Loss: 129.6995\n",
      "Epoch [75/1000], Loss: 128.4525\n",
      "Epoch [76/1000], Loss: 127.1909\n",
      "Epoch [77/1000], Loss: 125.9277\n",
      "Epoch [78/1000], Loss: 124.6781\n",
      "Epoch [79/1000], Loss: 123.4381\n",
      "Epoch [80/1000], Loss: 122.2025\n",
      "Epoch [81/1000], Loss: 120.9670\n",
      "Epoch [82/1000], Loss: 119.7259\n",
      "Epoch [83/1000], Loss: 118.4759\n",
      "Epoch [84/1000], Loss: 117.2149\n",
      "Epoch [85/1000], Loss: 115.9304\n",
      "Epoch [86/1000], Loss: 114.6253\n",
      "Epoch [87/1000], Loss: 113.2998\n",
      "Epoch [88/1000], Loss: 111.9507\n",
      "Epoch [89/1000], Loss: 110.5826\n",
      "Epoch [90/1000], Loss: 109.1905\n",
      "Epoch [91/1000], Loss: 107.7867\n",
      "Epoch [92/1000], Loss: 106.3761\n",
      "Epoch [93/1000], Loss: 104.9603\n",
      "Epoch [94/1000], Loss: 103.5334\n",
      "Epoch [95/1000], Loss: 102.1091\n",
      "Epoch [96/1000], Loss: 100.6740\n",
      "Epoch [97/1000], Loss: 99.2273\n",
      "Epoch [98/1000], Loss: 97.7682\n",
      "Epoch [99/1000], Loss: 96.2973\n",
      "Epoch [100/1000], Loss: 94.8229\n",
      "Epoch [101/1000], Loss: 93.3320\n",
      "Epoch [102/1000], Loss: 91.8308\n",
      "Epoch [103/1000], Loss: 90.3260\n",
      "Epoch [104/1000], Loss: 88.8126\n",
      "Epoch [105/1000], Loss: 87.2924\n",
      "Epoch [106/1000], Loss: 85.7668\n",
      "Epoch [107/1000], Loss: 84.2486\n",
      "Epoch [108/1000], Loss: 82.7214\n",
      "Epoch [109/1000], Loss: 81.1870\n",
      "Epoch [110/1000], Loss: 79.6627\n",
      "Epoch [111/1000], Loss: 78.1407\n",
      "Epoch [112/1000], Loss: 76.6224\n",
      "Epoch [113/1000], Loss: 75.0988\n",
      "Epoch [114/1000], Loss: 73.5797\n",
      "Epoch [115/1000], Loss: 72.0752\n",
      "Epoch [116/1000], Loss: 70.5643\n",
      "Epoch [117/1000], Loss: 69.0652\n",
      "Epoch [118/1000], Loss: 67.6226\n",
      "Epoch [119/1000], Loss: 66.1433\n",
      "Epoch [120/1000], Loss: 64.6645\n",
      "Epoch [121/1000], Loss: 63.2381\n",
      "Epoch [122/1000], Loss: 61.8172\n",
      "Epoch [123/1000], Loss: 60.4033\n",
      "Epoch [124/1000], Loss: 58.9901\n",
      "Epoch [125/1000], Loss: 57.5792\n",
      "Epoch [126/1000], Loss: 56.1602\n",
      "Epoch [127/1000], Loss: 54.7641\n",
      "Epoch [128/1000], Loss: 53.4482\n",
      "Epoch [129/1000], Loss: 52.0793\n",
      "Epoch [130/1000], Loss: 50.7452\n",
      "Epoch [131/1000], Loss: 49.4578\n",
      "Epoch [132/1000], Loss: 48.1822\n",
      "Epoch [133/1000], Loss: 46.9174\n",
      "Epoch [134/1000], Loss: 45.6625\n",
      "Epoch [135/1000], Loss: 44.4470\n",
      "Epoch [136/1000], Loss: 43.2581\n",
      "Epoch [137/1000], Loss: 42.0815\n",
      "Epoch [138/1000], Loss: 40.9200\n",
      "Epoch [139/1000], Loss: 39.7751\n",
      "Epoch [140/1000], Loss: 38.6536\n",
      "Epoch [141/1000], Loss: 37.5574\n",
      "Epoch [142/1000], Loss: 36.4899\n",
      "Epoch [143/1000], Loss: 35.4475\n",
      "Epoch [144/1000], Loss: 34.4314\n",
      "Epoch [145/1000], Loss: 33.4468\n",
      "Epoch [146/1000], Loss: 32.4914\n",
      "Epoch [147/1000], Loss: 31.5659\n",
      "Epoch [148/1000], Loss: 30.6706\n",
      "Epoch [149/1000], Loss: 29.8057\n",
      "Epoch [150/1000], Loss: 28.9708\n",
      "Epoch [151/1000], Loss: 28.1656\n",
      "Epoch [152/1000], Loss: 27.3933\n",
      "Epoch [153/1000], Loss: 26.6501\n",
      "Epoch [154/1000], Loss: 25.9347\n",
      "Epoch [155/1000], Loss: 25.2468\n",
      "Epoch [156/1000], Loss: 24.5899\n",
      "Epoch [157/1000], Loss: 23.9637\n",
      "Epoch [158/1000], Loss: 23.3656\n",
      "Epoch [159/1000], Loss: 22.7959\n",
      "Epoch [160/1000], Loss: 22.2548\n",
      "Epoch [161/1000], Loss: 21.7406\n",
      "Epoch [162/1000], Loss: 21.2537\n",
      "Epoch [163/1000], Loss: 20.7933\n",
      "Epoch [164/1000], Loss: 20.3570\n",
      "Epoch [165/1000], Loss: 19.9440\n",
      "Epoch [166/1000], Loss: 19.5537\n",
      "Epoch [167/1000], Loss: 19.1858\n",
      "Epoch [168/1000], Loss: 18.8391\n",
      "Epoch [169/1000], Loss: 18.5118\n",
      "Epoch [170/1000], Loss: 18.2013\n",
      "Epoch [171/1000], Loss: 17.9072\n",
      "Epoch [172/1000], Loss: 17.6565\n",
      "Epoch [173/1000], Loss: 17.4159\n",
      "Epoch [174/1000], Loss: 17.1717\n",
      "Epoch [175/1000], Loss: 16.9312\n",
      "Epoch [176/1000], Loss: 16.7326\n",
      "Epoch [177/1000], Loss: 16.5439\n",
      "Epoch [178/1000], Loss: 16.3734\n",
      "Epoch [179/1000], Loss: 16.2120\n",
      "Epoch [180/1000], Loss: 16.0608\n",
      "Epoch [181/1000], Loss: 15.9175\n",
      "Epoch [182/1000], Loss: 15.7835\n",
      "Epoch [183/1000], Loss: 15.6578\n",
      "Epoch [184/1000], Loss: 15.5432\n",
      "Epoch [185/1000], Loss: 15.4311\n",
      "Epoch [186/1000], Loss: 15.3290\n",
      "Epoch [187/1000], Loss: 15.2339\n",
      "Epoch [188/1000], Loss: 15.1453\n",
      "Epoch [189/1000], Loss: 15.0623\n",
      "Epoch [190/1000], Loss: 14.9841\n",
      "Epoch [191/1000], Loss: 14.9135\n",
      "Epoch [192/1000], Loss: 14.8415\n",
      "Epoch [193/1000], Loss: 14.7761\n",
      "Epoch [194/1000], Loss: 14.7143\n",
      "Epoch [195/1000], Loss: 14.6553\n",
      "Epoch [196/1000], Loss: 14.5993\n",
      "Epoch [197/1000], Loss: 14.5456\n",
      "Epoch [198/1000], Loss: 14.4940\n",
      "Epoch [199/1000], Loss: 14.4442\n",
      "Epoch [200/1000], Loss: 14.3960\n",
      "Epoch [201/1000], Loss: 14.3493\n",
      "Epoch [202/1000], Loss: 14.3039\n",
      "Epoch [203/1000], Loss: 14.2597\n",
      "Epoch [204/1000], Loss: 14.2166\n",
      "Epoch [205/1000], Loss: 14.1744\n",
      "Epoch [206/1000], Loss: 14.1331\n",
      "Epoch [207/1000], Loss: 14.0925\n",
      "Epoch [208/1000], Loss: 14.0543\n",
      "Epoch [209/1000], Loss: 14.0144\n",
      "Epoch [210/1000], Loss: 13.9768\n",
      "Epoch [211/1000], Loss: 13.9397\n",
      "Epoch [212/1000], Loss: 13.9029\n",
      "Epoch [213/1000], Loss: 13.8667\n",
      "Epoch [214/1000], Loss: 13.8312\n",
      "Epoch [215/1000], Loss: 13.7963\n",
      "Epoch [216/1000], Loss: 13.7620\n",
      "Epoch [217/1000], Loss: 13.7314\n",
      "Epoch [218/1000], Loss: 13.6978\n",
      "Epoch [219/1000], Loss: 13.6644\n",
      "Epoch [220/1000], Loss: 13.6337\n",
      "Epoch [221/1000], Loss: 13.6033\n",
      "Epoch [222/1000], Loss: 13.5728\n",
      "Epoch [223/1000], Loss: 13.5426\n",
      "Epoch [224/1000], Loss: 13.5129\n",
      "Epoch [225/1000], Loss: 13.4840\n",
      "Epoch [226/1000], Loss: 13.4558\n",
      "Epoch [227/1000], Loss: 13.4278\n",
      "Epoch [228/1000], Loss: 13.3997\n",
      "Epoch [229/1000], Loss: 13.3720\n",
      "Epoch [230/1000], Loss: 13.3444\n",
      "Epoch [231/1000], Loss: 13.3172\n",
      "Epoch [232/1000], Loss: 13.2910\n",
      "Epoch [233/1000], Loss: 13.2655\n",
      "Epoch [234/1000], Loss: 13.2411\n",
      "Epoch [235/1000], Loss: 13.2169\n",
      "Epoch [236/1000], Loss: 13.1944\n",
      "Epoch [237/1000], Loss: 13.1684\n",
      "Epoch [238/1000], Loss: 13.1443\n",
      "Epoch [239/1000], Loss: 13.1204\n",
      "Epoch [240/1000], Loss: 13.0971\n",
      "Epoch [241/1000], Loss: 13.0743\n",
      "Epoch [242/1000], Loss: 13.0520\n",
      "Epoch [243/1000], Loss: 13.0309\n",
      "Epoch [244/1000], Loss: 13.0074\n",
      "Epoch [245/1000], Loss: 12.9857\n",
      "Epoch [246/1000], Loss: 12.9644\n",
      "Epoch [247/1000], Loss: 12.9430\n",
      "Epoch [248/1000], Loss: 12.9215\n",
      "Epoch [249/1000], Loss: 12.9001\n",
      "Epoch [250/1000], Loss: 12.8796\n",
      "Epoch [251/1000], Loss: 12.8593\n",
      "Epoch [252/1000], Loss: 12.8392\n",
      "Epoch [253/1000], Loss: 12.8193\n",
      "Epoch [254/1000], Loss: 12.7994\n",
      "Epoch [255/1000], Loss: 12.7795\n",
      "Epoch [256/1000], Loss: 12.7598\n",
      "Epoch [257/1000], Loss: 12.7403\n",
      "Epoch [258/1000], Loss: 12.7211\n",
      "Epoch [259/1000], Loss: 12.7037\n",
      "Epoch [260/1000], Loss: 12.6839\n",
      "Epoch [261/1000], Loss: 12.6658\n",
      "Epoch [262/1000], Loss: 12.6476\n",
      "Epoch [263/1000], Loss: 12.6292\n",
      "Epoch [264/1000], Loss: 12.6108\n",
      "Epoch [265/1000], Loss: 12.5926\n",
      "Epoch [266/1000], Loss: 12.5742\n",
      "Epoch [267/1000], Loss: 12.5556\n",
      "Epoch [268/1000], Loss: 12.5369\n",
      "Epoch [269/1000], Loss: 12.5180\n",
      "Epoch [270/1000], Loss: 12.4992\n",
      "Epoch [271/1000], Loss: 12.4806\n",
      "Epoch [272/1000], Loss: 12.4648\n",
      "Epoch [273/1000], Loss: 12.4454\n",
      "Epoch [274/1000], Loss: 12.4291\n",
      "Epoch [275/1000], Loss: 12.4122\n",
      "Epoch [276/1000], Loss: 12.3952\n",
      "Epoch [277/1000], Loss: 12.3784\n",
      "Epoch [278/1000], Loss: 12.3618\n",
      "Epoch [279/1000], Loss: 12.3454\n",
      "Epoch [280/1000], Loss: 12.3287\n",
      "Epoch [281/1000], Loss: 12.3119\n",
      "Epoch [282/1000], Loss: 12.2953\n",
      "Epoch [283/1000], Loss: 12.2796\n",
      "Epoch [284/1000], Loss: 12.2634\n",
      "Epoch [285/1000], Loss: 12.2483\n",
      "Epoch [286/1000], Loss: 12.2334\n",
      "Epoch [287/1000], Loss: 12.2173\n",
      "Epoch [288/1000], Loss: 12.2011\n",
      "Epoch [289/1000], Loss: 12.1850\n",
      "Epoch [290/1000], Loss: 12.1692\n",
      "Epoch [291/1000], Loss: 12.1540\n",
      "Epoch [292/1000], Loss: 12.1396\n",
      "Epoch [293/1000], Loss: 12.1253\n",
      "Epoch [294/1000], Loss: 12.1110\n",
      "Epoch [295/1000], Loss: 12.0966\n",
      "Epoch [296/1000], Loss: 12.0826\n",
      "Epoch [297/1000], Loss: 12.0688\n",
      "Epoch [298/1000], Loss: 12.0552\n",
      "Epoch [299/1000], Loss: 12.0415\n",
      "Epoch [300/1000], Loss: 12.0276\n",
      "Epoch [301/1000], Loss: 12.0138\n",
      "Epoch [302/1000], Loss: 12.0001\n",
      "Epoch [303/1000], Loss: 11.9866\n",
      "Epoch [304/1000], Loss: 11.9735\n",
      "Epoch [305/1000], Loss: 11.9635\n",
      "Epoch [306/1000], Loss: 11.9476\n",
      "Epoch [307/1000], Loss: 11.9372\n",
      "Epoch [308/1000], Loss: 11.9255\n",
      "Epoch [309/1000], Loss: 11.9134\n",
      "Epoch [310/1000], Loss: 11.8997\n",
      "Epoch [311/1000], Loss: 11.8898\n",
      "Epoch [312/1000], Loss: 11.8769\n",
      "Epoch [313/1000], Loss: 11.8611\n",
      "Epoch [314/1000], Loss: 11.8496\n",
      "Epoch [315/1000], Loss: 11.8382\n",
      "Epoch [316/1000], Loss: 11.8276\n",
      "Epoch [317/1000], Loss: 11.8148\n",
      "Epoch [318/1000], Loss: 11.8041\n",
      "Epoch [319/1000], Loss: 11.7930\n",
      "Epoch [320/1000], Loss: 11.7808\n",
      "Epoch [321/1000], Loss: 11.7683\n",
      "Epoch [322/1000], Loss: 11.7570\n",
      "Epoch [323/1000], Loss: 11.7478\n",
      "Epoch [324/1000], Loss: 11.7337\n",
      "Epoch [325/1000], Loss: 11.7227\n",
      "Epoch [326/1000], Loss: 11.7118\n",
      "Epoch [327/1000], Loss: 11.7006\n",
      "Epoch [328/1000], Loss: 11.6888\n",
      "Epoch [329/1000], Loss: 11.6768\n",
      "Epoch [330/1000], Loss: 11.6650\n",
      "Epoch [331/1000], Loss: 11.6534\n",
      "Epoch [332/1000], Loss: 11.6417\n",
      "Epoch [333/1000], Loss: 11.6297\n",
      "Epoch [334/1000], Loss: 11.6193\n",
      "Epoch [335/1000], Loss: 11.6057\n",
      "Epoch [336/1000], Loss: 11.5957\n",
      "Epoch [337/1000], Loss: 11.5850\n",
      "Epoch [338/1000], Loss: 11.5730\n",
      "Epoch [339/1000], Loss: 11.5595\n",
      "Epoch [340/1000], Loss: 11.5511\n",
      "Epoch [341/1000], Loss: 11.5385\n",
      "Epoch [342/1000], Loss: 11.5282\n",
      "Epoch [343/1000], Loss: 11.5153\n",
      "Epoch [344/1000], Loss: 11.5040\n",
      "Epoch [345/1000], Loss: 11.4913\n",
      "Epoch [346/1000], Loss: 11.4833\n",
      "Epoch [347/1000], Loss: 11.4669\n",
      "Epoch [348/1000], Loss: 11.4562\n",
      "Epoch [349/1000], Loss: 11.4450\n",
      "Epoch [350/1000], Loss: 11.4355\n",
      "Epoch [351/1000], Loss: 11.4250\n",
      "Epoch [352/1000], Loss: 11.4150\n",
      "Epoch [353/1000], Loss: 11.4035\n",
      "Epoch [354/1000], Loss: 11.3922\n",
      "Epoch [355/1000], Loss: 11.3799\n",
      "Epoch [356/1000], Loss: 11.3711\n",
      "Epoch [357/1000], Loss: 11.3573\n",
      "Epoch [358/1000], Loss: 11.3465\n",
      "Epoch [359/1000], Loss: 11.3356\n",
      "Epoch [360/1000], Loss: 11.3243\n",
      "Epoch [361/1000], Loss: 11.3127\n",
      "Epoch [362/1000], Loss: 11.3022\n",
      "Epoch [363/1000], Loss: 11.2908\n",
      "Epoch [364/1000], Loss: 11.2805\n",
      "Epoch [365/1000], Loss: 11.2720\n",
      "Epoch [366/1000], Loss: 11.2572\n",
      "Epoch [367/1000], Loss: 11.2517\n",
      "Epoch [368/1000], Loss: 11.2374\n",
      "Epoch [369/1000], Loss: 11.2299\n",
      "Epoch [370/1000], Loss: 11.2205\n",
      "Epoch [371/1000], Loss: 11.2074\n",
      "Epoch [372/1000], Loss: 11.1939\n",
      "Epoch [373/1000], Loss: 11.1895\n",
      "Epoch [374/1000], Loss: 11.1774\n",
      "Epoch [375/1000], Loss: 11.1639\n",
      "Epoch [376/1000], Loss: 11.1566\n",
      "Epoch [377/1000], Loss: 11.1476\n",
      "Epoch [378/1000], Loss: 11.1358\n",
      "Epoch [379/1000], Loss: 11.1213\n",
      "Epoch [380/1000], Loss: 11.1073\n",
      "Epoch [381/1000], Loss: 11.0958\n",
      "Epoch [382/1000], Loss: 11.0830\n",
      "Epoch [383/1000], Loss: 11.0718\n",
      "Epoch [384/1000], Loss: 11.0578\n",
      "Epoch [385/1000], Loss: 11.0473\n",
      "Epoch [386/1000], Loss: 11.0352\n",
      "Epoch [387/1000], Loss: 11.0236\n",
      "Epoch [388/1000], Loss: 11.0097\n",
      "Epoch [389/1000], Loss: 10.9952\n",
      "Epoch [390/1000], Loss: 10.9808\n",
      "Epoch [391/1000], Loss: 10.9681\n",
      "Epoch [392/1000], Loss: 10.9543\n",
      "Epoch [393/1000], Loss: 10.9402\n",
      "Epoch [394/1000], Loss: 10.9272\n",
      "Epoch [395/1000], Loss: 10.9117\n",
      "Epoch [396/1000], Loss: 10.8966\n",
      "Epoch [397/1000], Loss: 10.8837\n",
      "Epoch [398/1000], Loss: 10.8696\n",
      "Epoch [399/1000], Loss: 10.8544\n",
      "Epoch [400/1000], Loss: 10.8362\n",
      "Epoch [401/1000], Loss: 10.8259\n",
      "Epoch [402/1000], Loss: 10.8097\n",
      "Epoch [403/1000], Loss: 10.7869\n",
      "Epoch [404/1000], Loss: 10.7755\n",
      "Epoch [405/1000], Loss: 10.7605\n",
      "Epoch [406/1000], Loss: 10.7419\n",
      "Epoch [407/1000], Loss: 10.7205\n",
      "Epoch [408/1000], Loss: 10.7041\n",
      "Epoch [409/1000], Loss: 10.6863\n",
      "Epoch [410/1000], Loss: 10.6656\n",
      "Epoch [411/1000], Loss: 10.6476\n",
      "Epoch [412/1000], Loss: 10.6303\n",
      "Epoch [413/1000], Loss: 10.6137\n",
      "Epoch [414/1000], Loss: 10.5999\n",
      "Epoch [415/1000], Loss: 10.5820\n",
      "Epoch [416/1000], Loss: 10.5666\n",
      "Epoch [417/1000], Loss: 10.5492\n",
      "Epoch [418/1000], Loss: 10.5288\n",
      "Epoch [419/1000], Loss: 10.5114\n",
      "Epoch [420/1000], Loss: 10.4922\n",
      "Epoch [421/1000], Loss: 10.4748\n",
      "Epoch [422/1000], Loss: 10.4591\n",
      "Epoch [423/1000], Loss: 10.4390\n",
      "Epoch [424/1000], Loss: 10.4245\n",
      "Epoch [425/1000], Loss: 10.4085\n",
      "Epoch [426/1000], Loss: 10.3887\n",
      "Epoch [427/1000], Loss: 10.3717\n",
      "Epoch [428/1000], Loss: 10.3536\n",
      "Epoch [429/1000], Loss: 10.3346\n",
      "Epoch [430/1000], Loss: 10.3176\n",
      "Epoch [431/1000], Loss: 10.3024\n",
      "Epoch [432/1000], Loss: 10.2807\n",
      "Epoch [433/1000], Loss: 10.2645\n",
      "Epoch [434/1000], Loss: 10.2483\n",
      "Epoch [435/1000], Loss: 10.2283\n",
      "Epoch [436/1000], Loss: 10.2144\n",
      "Epoch [437/1000], Loss: 10.1994\n",
      "Epoch [438/1000], Loss: 10.1799\n",
      "Epoch [439/1000], Loss: 10.1591\n",
      "Epoch [440/1000], Loss: 10.1432\n",
      "Epoch [441/1000], Loss: 10.1275\n",
      "Epoch [442/1000], Loss: 10.1080\n",
      "Epoch [443/1000], Loss: 10.0946\n",
      "Epoch [444/1000], Loss: 10.0812\n",
      "Epoch [445/1000], Loss: 10.0599\n",
      "Epoch [446/1000], Loss: 10.0402\n",
      "Epoch [447/1000], Loss: 10.0221\n",
      "Epoch [448/1000], Loss: 10.0155\n",
      "Epoch [449/1000], Loss: 9.9960\n",
      "Epoch [450/1000], Loss: 9.9739\n",
      "Epoch [451/1000], Loss: 9.9612\n",
      "Epoch [452/1000], Loss: 9.9460\n",
      "Epoch [453/1000], Loss: 9.9286\n",
      "Epoch [454/1000], Loss: 9.9098\n",
      "Epoch [455/1000], Loss: 9.8972\n",
      "Epoch [456/1000], Loss: 9.8832\n",
      "Epoch [457/1000], Loss: 9.8628\n",
      "Epoch [458/1000], Loss: 9.8497\n",
      "Epoch [459/1000], Loss: 9.8341\n",
      "Epoch [460/1000], Loss: 9.8166\n",
      "Epoch [461/1000], Loss: 9.7988\n",
      "Epoch [462/1000], Loss: 9.7883\n",
      "Epoch [463/1000], Loss: 9.7710\n",
      "Epoch [464/1000], Loss: 9.7565\n",
      "Epoch [465/1000], Loss: 9.7425\n",
      "Epoch [466/1000], Loss: 9.7282\n",
      "Epoch [467/1000], Loss: 9.7146\n",
      "Epoch [468/1000], Loss: 9.6951\n",
      "Epoch [469/1000], Loss: 9.6786\n",
      "Epoch [470/1000], Loss: 9.6686\n",
      "Epoch [471/1000], Loss: 9.6508\n",
      "Epoch [472/1000], Loss: 9.6363\n",
      "Epoch [473/1000], Loss: 9.6249\n",
      "Epoch [474/1000], Loss: 9.6095\n",
      "Epoch [475/1000], Loss: 9.5912\n",
      "Epoch [476/1000], Loss: 9.5747\n",
      "Epoch [477/1000], Loss: 9.5663\n",
      "Epoch [478/1000], Loss: 9.5469\n",
      "Epoch [479/1000], Loss: 9.5298\n",
      "Epoch [480/1000], Loss: 9.5179\n",
      "Epoch [481/1000], Loss: 9.5034\n",
      "Epoch [482/1000], Loss: 9.4866\n",
      "Epoch [483/1000], Loss: 9.4705\n",
      "Epoch [484/1000], Loss: 9.4568\n",
      "Epoch [485/1000], Loss: 9.4442\n",
      "Epoch [486/1000], Loss: 9.4264\n",
      "Epoch [487/1000], Loss: 9.4096\n",
      "Epoch [488/1000], Loss: 9.3947\n",
      "Epoch [489/1000], Loss: 9.3815\n",
      "Epoch [490/1000], Loss: 9.3691\n",
      "Epoch [491/1000], Loss: 9.3516\n",
      "Epoch [492/1000], Loss: 9.3326\n",
      "Epoch [493/1000], Loss: 9.3170\n",
      "Epoch [494/1000], Loss: 9.3018\n",
      "Epoch [495/1000], Loss: 9.2884\n",
      "Epoch [496/1000], Loss: 9.2729\n",
      "Epoch [497/1000], Loss: 9.2595\n",
      "Epoch [498/1000], Loss: 9.2441\n",
      "Epoch [499/1000], Loss: 9.2281\n",
      "Epoch [500/1000], Loss: 9.2115\n",
      "Epoch [501/1000], Loss: 9.1954\n",
      "Epoch [502/1000], Loss: 9.1806\n",
      "Epoch [503/1000], Loss: 9.1652\n",
      "Epoch [504/1000], Loss: 9.1493\n",
      "Epoch [505/1000], Loss: 9.1345\n",
      "Epoch [506/1000], Loss: 9.1199\n",
      "Epoch [507/1000], Loss: 9.1041\n",
      "Epoch [508/1000], Loss: 9.0877\n",
      "Epoch [509/1000], Loss: 9.0741\n",
      "Epoch [510/1000], Loss: 9.0559\n",
      "Epoch [511/1000], Loss: 9.0399\n",
      "Epoch [512/1000], Loss: 9.0262\n",
      "Epoch [513/1000], Loss: 9.0090\n",
      "Epoch [514/1000], Loss: 8.9948\n",
      "Epoch [515/1000], Loss: 8.9789\n",
      "Epoch [516/1000], Loss: 8.9611\n",
      "Epoch [517/1000], Loss: 8.9459\n",
      "Epoch [518/1000], Loss: 8.9290\n",
      "Epoch [519/1000], Loss: 8.9138\n",
      "Epoch [520/1000], Loss: 8.8981\n",
      "Epoch [521/1000], Loss: 8.8819\n",
      "Epoch [522/1000], Loss: 8.8683\n",
      "Epoch [523/1000], Loss: 8.8517\n",
      "Epoch [524/1000], Loss: 8.8370\n",
      "Epoch [525/1000], Loss: 8.8228\n",
      "Epoch [526/1000], Loss: 8.8078\n",
      "Epoch [527/1000], Loss: 8.7925\n",
      "Epoch [528/1000], Loss: 8.7811\n",
      "Epoch [529/1000], Loss: 8.7625\n",
      "Epoch [530/1000], Loss: 8.7488\n",
      "Epoch [531/1000], Loss: 8.7360\n",
      "Epoch [532/1000], Loss: 8.7197\n",
      "Epoch [533/1000], Loss: 8.7042\n",
      "Epoch [534/1000], Loss: 8.6885\n",
      "Epoch [535/1000], Loss: 8.6750\n",
      "Epoch [536/1000], Loss: 8.6571\n",
      "Epoch [537/1000], Loss: 8.6419\n",
      "Epoch [538/1000], Loss: 8.6270\n",
      "Epoch [539/1000], Loss: 8.6110\n",
      "Epoch [540/1000], Loss: 8.5939\n",
      "Epoch [541/1000], Loss: 8.5776\n",
      "Epoch [542/1000], Loss: 8.5637\n",
      "Epoch [543/1000], Loss: 8.5444\n",
      "Epoch [544/1000], Loss: 8.5289\n",
      "Epoch [545/1000], Loss: 8.5146\n",
      "Epoch [546/1000], Loss: 8.4973\n",
      "Epoch [547/1000], Loss: 8.4809\n",
      "Epoch [548/1000], Loss: 8.4654\n",
      "Epoch [549/1000], Loss: 8.4512\n",
      "Epoch [550/1000], Loss: 8.4345\n",
      "Epoch [551/1000], Loss: 8.4193\n",
      "Epoch [552/1000], Loss: 8.4052\n",
      "Epoch [553/1000], Loss: 8.3887\n",
      "Epoch [554/1000], Loss: 8.3724\n",
      "Epoch [555/1000], Loss: 8.3555\n",
      "Epoch [556/1000], Loss: 8.3391\n",
      "Epoch [557/1000], Loss: 8.3234\n",
      "Epoch [558/1000], Loss: 8.3068\n",
      "Epoch [559/1000], Loss: 8.2898\n",
      "Epoch [560/1000], Loss: 8.2727\n",
      "Epoch [561/1000], Loss: 8.2558\n",
      "Epoch [562/1000], Loss: 8.2395\n",
      "Epoch [563/1000], Loss: 8.2230\n",
      "Epoch [564/1000], Loss: 8.2069\n",
      "Epoch [565/1000], Loss: 8.1929\n",
      "Epoch [566/1000], Loss: 8.1751\n",
      "Epoch [567/1000], Loss: 8.1583\n",
      "Epoch [568/1000], Loss: 8.1410\n",
      "Epoch [569/1000], Loss: 8.1249\n",
      "Epoch [570/1000], Loss: 8.1085\n",
      "Epoch [571/1000], Loss: 8.0910\n",
      "Epoch [572/1000], Loss: 8.0731\n",
      "Epoch [573/1000], Loss: 8.0560\n",
      "Epoch [574/1000], Loss: 8.0389\n",
      "Epoch [575/1000], Loss: 8.0212\n",
      "Epoch [576/1000], Loss: 8.0031\n",
      "Epoch [577/1000], Loss: 7.9849\n",
      "Epoch [578/1000], Loss: 7.9668\n",
      "Epoch [579/1000], Loss: 7.9491\n",
      "Epoch [580/1000], Loss: 7.9306\n",
      "Epoch [581/1000], Loss: 7.9116\n",
      "Epoch [582/1000], Loss: 7.8935\n",
      "Epoch [583/1000], Loss: 7.8766\n",
      "Epoch [584/1000], Loss: 7.8571\n",
      "Epoch [585/1000], Loss: 7.8379\n",
      "Epoch [586/1000], Loss: 7.8212\n",
      "Epoch [587/1000], Loss: 7.8004\n",
      "Epoch [588/1000], Loss: 7.7821\n",
      "Epoch [589/1000], Loss: 7.7631\n",
      "Epoch [590/1000], Loss: 7.7437\n",
      "Epoch [591/1000], Loss: 7.7273\n",
      "Epoch [592/1000], Loss: 7.7060\n",
      "Epoch [593/1000], Loss: 7.6874\n",
      "Epoch [594/1000], Loss: 7.6698\n",
      "Epoch [595/1000], Loss: 7.6491\n",
      "Epoch [596/1000], Loss: 7.6310\n",
      "Epoch [597/1000], Loss: 7.6131\n",
      "Epoch [598/1000], Loss: 7.5950\n",
      "Epoch [599/1000], Loss: 7.5758\n",
      "Epoch [600/1000], Loss: 7.5581\n",
      "Epoch [601/1000], Loss: 7.5383\n",
      "Epoch [602/1000], Loss: 7.5200\n",
      "Epoch [603/1000], Loss: 7.5012\n",
      "Epoch [604/1000], Loss: 7.4823\n",
      "Epoch [605/1000], Loss: 7.4644\n",
      "Epoch [606/1000], Loss: 7.4456\n",
      "Epoch [607/1000], Loss: 7.4279\n",
      "Epoch [608/1000], Loss: 7.4098\n",
      "Epoch [609/1000], Loss: 7.3915\n",
      "Epoch [610/1000], Loss: 7.3746\n",
      "Epoch [611/1000], Loss: 7.3551\n",
      "Epoch [612/1000], Loss: 7.3397\n",
      "Epoch [613/1000], Loss: 7.3202\n",
      "Epoch [614/1000], Loss: 7.3040\n",
      "Epoch [615/1000], Loss: 7.2863\n",
      "Epoch [616/1000], Loss: 7.2677\n",
      "Epoch [617/1000], Loss: 7.2524\n",
      "Epoch [618/1000], Loss: 7.2323\n",
      "Epoch [619/1000], Loss: 7.2151\n",
      "Epoch [620/1000], Loss: 7.1981\n",
      "Epoch [621/1000], Loss: 7.1831\n",
      "Epoch [622/1000], Loss: 7.1648\n",
      "Epoch [623/1000], Loss: 7.1490\n",
      "Epoch [624/1000], Loss: 7.1334\n",
      "Epoch [625/1000], Loss: 7.1167\n",
      "Epoch [626/1000], Loss: 7.0998\n",
      "Epoch [627/1000], Loss: 7.0859\n",
      "Epoch [628/1000], Loss: 7.0676\n",
      "Epoch [629/1000], Loss: 7.0508\n",
      "Epoch [630/1000], Loss: 7.0347\n",
      "Epoch [631/1000], Loss: 7.0176\n",
      "Epoch [632/1000], Loss: 7.0012\n",
      "Epoch [633/1000], Loss: 6.9840\n",
      "Epoch [634/1000], Loss: 6.9678\n",
      "Epoch [635/1000], Loss: 6.9513\n",
      "Epoch [636/1000], Loss: 6.9347\n",
      "Epoch [637/1000], Loss: 6.9190\n",
      "Epoch [638/1000], Loss: 6.9019\n",
      "Epoch [639/1000], Loss: 6.8872\n",
      "Epoch [640/1000], Loss: 6.8698\n",
      "Epoch [641/1000], Loss: 6.8525\n",
      "Epoch [642/1000], Loss: 6.8351\n",
      "Epoch [643/1000], Loss: 6.8202\n",
      "Epoch [644/1000], Loss: 6.8013\n",
      "Epoch [645/1000], Loss: 6.7845\n",
      "Epoch [646/1000], Loss: 6.7675\n",
      "Epoch [647/1000], Loss: 6.7504\n",
      "Epoch [648/1000], Loss: 6.7327\n",
      "Epoch [649/1000], Loss: 6.7194\n",
      "Epoch [650/1000], Loss: 6.6985\n",
      "Epoch [651/1000], Loss: 6.6822\n",
      "Epoch [652/1000], Loss: 6.6653\n",
      "Epoch [653/1000], Loss: 6.6481\n",
      "Epoch [654/1000], Loss: 6.6312\n",
      "Epoch [655/1000], Loss: 6.6140\n",
      "Epoch [656/1000], Loss: 6.5974\n",
      "Epoch [657/1000], Loss: 6.5800\n",
      "Epoch [658/1000], Loss: 6.5634\n",
      "Epoch [659/1000], Loss: 6.5448\n",
      "Epoch [660/1000], Loss: 6.5296\n",
      "Epoch [661/1000], Loss: 6.5112\n",
      "Epoch [662/1000], Loss: 6.4946\n",
      "Epoch [663/1000], Loss: 6.4771\n",
      "Epoch [664/1000], Loss: 6.4594\n",
      "Epoch [665/1000], Loss: 6.4420\n",
      "Epoch [666/1000], Loss: 6.4275\n",
      "Epoch [667/1000], Loss: 6.4080\n",
      "Epoch [668/1000], Loss: 6.3923\n",
      "Epoch [669/1000], Loss: 6.3753\n",
      "Epoch [670/1000], Loss: 6.3578\n",
      "Epoch [671/1000], Loss: 6.3400\n",
      "Epoch [672/1000], Loss: 6.3254\n",
      "Epoch [673/1000], Loss: 6.3061\n",
      "Epoch [674/1000], Loss: 6.2893\n",
      "Epoch [675/1000], Loss: 6.2723\n",
      "Epoch [676/1000], Loss: 6.2548\n",
      "Epoch [677/1000], Loss: 6.2375\n",
      "Epoch [678/1000], Loss: 6.2210\n",
      "Epoch [679/1000], Loss: 6.2042\n",
      "Epoch [680/1000], Loss: 6.1884\n",
      "Epoch [681/1000], Loss: 6.1713\n",
      "Epoch [682/1000], Loss: 6.1536\n",
      "Epoch [683/1000], Loss: 6.1363\n",
      "Epoch [684/1000], Loss: 6.1205\n",
      "Epoch [685/1000], Loss: 6.1020\n",
      "Epoch [686/1000], Loss: 6.0857\n",
      "Epoch [687/1000], Loss: 6.0692\n",
      "Epoch [688/1000], Loss: 6.0520\n",
      "Epoch [689/1000], Loss: 6.0343\n",
      "Epoch [690/1000], Loss: 6.0174\n",
      "Epoch [691/1000], Loss: 6.0015\n",
      "Epoch [692/1000], Loss: 5.9839\n",
      "Epoch [693/1000], Loss: 5.9676\n",
      "Epoch [694/1000], Loss: 5.9509\n",
      "Epoch [695/1000], Loss: 5.9334\n",
      "Epoch [696/1000], Loss: 5.9159\n",
      "Epoch [697/1000], Loss: 5.9007\n",
      "Epoch [698/1000], Loss: 5.8817\n",
      "Epoch [699/1000], Loss: 5.8650\n",
      "Epoch [700/1000], Loss: 5.8494\n",
      "Epoch [701/1000], Loss: 5.8317\n",
      "Epoch [702/1000], Loss: 5.8142\n",
      "Epoch [703/1000], Loss: 5.7990\n",
      "Epoch [704/1000], Loss: 5.7806\n",
      "Epoch [705/1000], Loss: 5.7640\n",
      "Epoch [706/1000], Loss: 5.7473\n",
      "Epoch [707/1000], Loss: 5.7301\n",
      "Epoch [708/1000], Loss: 5.7129\n",
      "Epoch [709/1000], Loss: 5.6977\n",
      "Epoch [710/1000], Loss: 5.6795\n",
      "Epoch [711/1000], Loss: 5.6634\n",
      "Epoch [712/1000], Loss: 5.6464\n",
      "Epoch [713/1000], Loss: 5.6291\n",
      "Epoch [714/1000], Loss: 5.6146\n",
      "Epoch [715/1000], Loss: 5.5956\n",
      "Epoch [716/1000], Loss: 5.5789\n",
      "Epoch [717/1000], Loss: 5.5622\n",
      "Epoch [718/1000], Loss: 5.5451\n",
      "Epoch [719/1000], Loss: 5.5287\n",
      "Epoch [720/1000], Loss: 5.5127\n",
      "Epoch [721/1000], Loss: 5.4948\n",
      "Epoch [722/1000], Loss: 5.4784\n",
      "Epoch [723/1000], Loss: 5.4613\n",
      "Epoch [724/1000], Loss: 5.4439\n",
      "Epoch [725/1000], Loss: 5.4286\n",
      "Epoch [726/1000], Loss: 5.4099\n",
      "Epoch [727/1000], Loss: 5.3930\n",
      "Epoch [728/1000], Loss: 5.3760\n",
      "Epoch [729/1000], Loss: 5.3598\n",
      "Epoch [730/1000], Loss: 5.3415\n",
      "Epoch [731/1000], Loss: 5.3250\n",
      "Epoch [732/1000], Loss: 5.3082\n",
      "Epoch [733/1000], Loss: 5.2908\n",
      "Epoch [734/1000], Loss: 5.2734\n",
      "Epoch [735/1000], Loss: 5.2575\n",
      "Epoch [736/1000], Loss: 5.2389\n",
      "Epoch [737/1000], Loss: 5.2227\n",
      "Epoch [738/1000], Loss: 5.2052\n",
      "Epoch [739/1000], Loss: 5.1878\n",
      "Epoch [740/1000], Loss: 5.1705\n",
      "Epoch [741/1000], Loss: 5.1531\n",
      "Epoch [742/1000], Loss: 5.1359\n",
      "Epoch [743/1000], Loss: 5.1187\n",
      "Epoch [744/1000], Loss: 5.1020\n",
      "Epoch [745/1000], Loss: 5.0845\n",
      "Epoch [746/1000], Loss: 5.0686\n",
      "Epoch [747/1000], Loss: 5.0509\n",
      "Epoch [748/1000], Loss: 5.0334\n",
      "Epoch [749/1000], Loss: 5.0159\n",
      "Epoch [750/1000], Loss: 5.0003\n",
      "Epoch [751/1000], Loss: 4.9815\n",
      "Epoch [752/1000], Loss: 4.9645\n",
      "Epoch [753/1000], Loss: 4.9473\n",
      "Epoch [754/1000], Loss: 4.9303\n",
      "Epoch [755/1000], Loss: 4.9132\n",
      "Epoch [756/1000], Loss: 4.8953\n",
      "Epoch [757/1000], Loss: 4.8771\n",
      "Epoch [758/1000], Loss: 4.8604\n",
      "Epoch [759/1000], Loss: 4.8409\n",
      "Epoch [760/1000], Loss: 4.8228\n",
      "Epoch [761/1000], Loss: 4.8045\n",
      "Epoch [762/1000], Loss: 4.7862\n",
      "Epoch [763/1000], Loss: 4.7691\n",
      "Epoch [764/1000], Loss: 4.7496\n",
      "Epoch [765/1000], Loss: 4.7329\n",
      "Epoch [766/1000], Loss: 4.7150\n",
      "Epoch [767/1000], Loss: 4.6956\n",
      "Epoch [768/1000], Loss: 4.6777\n",
      "Epoch [769/1000], Loss: 4.6588\n",
      "Epoch [770/1000], Loss: 4.6399\n",
      "Epoch [771/1000], Loss: 4.6208\n",
      "Epoch [772/1000], Loss: 4.6031\n",
      "Epoch [773/1000], Loss: 4.5846\n",
      "Epoch [774/1000], Loss: 4.5645\n",
      "Epoch [775/1000], Loss: 4.5466\n",
      "Epoch [776/1000], Loss: 4.5277\n",
      "Epoch [777/1000], Loss: 4.5080\n",
      "Epoch [778/1000], Loss: 4.4900\n",
      "Epoch [779/1000], Loss: 4.4712\n",
      "Epoch [780/1000], Loss: 4.4520\n",
      "Epoch [781/1000], Loss: 4.4331\n",
      "Epoch [782/1000], Loss: 4.4144\n",
      "Epoch [783/1000], Loss: 4.3951\n",
      "Epoch [784/1000], Loss: 4.3770\n",
      "Epoch [785/1000], Loss: 4.3581\n",
      "Epoch [786/1000], Loss: 4.3376\n",
      "Epoch [787/1000], Loss: 4.3193\n",
      "Epoch [788/1000], Loss: 4.2996\n",
      "Epoch [789/1000], Loss: 4.2785\n",
      "Epoch [790/1000], Loss: 4.2592\n",
      "Epoch [791/1000], Loss: 4.2402\n",
      "Epoch [792/1000], Loss: 4.2196\n",
      "Epoch [793/1000], Loss: 4.1998\n",
      "Epoch [794/1000], Loss: 4.1818\n",
      "Epoch [795/1000], Loss: 4.1626\n",
      "Epoch [796/1000], Loss: 4.1442\n",
      "Epoch [797/1000], Loss: 4.1254\n",
      "Epoch [798/1000], Loss: 4.1080\n",
      "Epoch [799/1000], Loss: 4.0885\n",
      "Epoch [800/1000], Loss: 4.0708\n",
      "Epoch [801/1000], Loss: 4.0528\n",
      "Epoch [802/1000], Loss: 4.0342\n",
      "Epoch [803/1000], Loss: 4.0160\n",
      "Epoch [804/1000], Loss: 3.9981\n",
      "Epoch [805/1000], Loss: 3.9799\n",
      "Epoch [806/1000], Loss: 3.9621\n",
      "Epoch [807/1000], Loss: 3.9432\n",
      "Epoch [808/1000], Loss: 3.9255\n",
      "Epoch [809/1000], Loss: 3.9082\n",
      "Epoch [810/1000], Loss: 3.8905\n",
      "Epoch [811/1000], Loss: 3.8736\n",
      "Epoch [812/1000], Loss: 3.8561\n",
      "Epoch [813/1000], Loss: 3.8397\n",
      "Epoch [814/1000], Loss: 3.8224\n",
      "Epoch [815/1000], Loss: 3.8055\n",
      "Epoch [816/1000], Loss: 3.7884\n",
      "Epoch [817/1000], Loss: 3.7722\n",
      "Epoch [818/1000], Loss: 3.7555\n",
      "Epoch [819/1000], Loss: 3.7391\n",
      "Epoch [820/1000], Loss: 3.7223\n",
      "Epoch [821/1000], Loss: 3.7048\n",
      "Epoch [822/1000], Loss: 3.6884\n",
      "Epoch [823/1000], Loss: 3.6720\n",
      "Epoch [824/1000], Loss: 3.6557\n",
      "Epoch [825/1000], Loss: 3.6394\n",
      "Epoch [826/1000], Loss: 3.6229\n",
      "Epoch [827/1000], Loss: 3.6070\n",
      "Epoch [828/1000], Loss: 3.5900\n",
      "Epoch [829/1000], Loss: 3.5736\n",
      "Epoch [830/1000], Loss: 3.5570\n",
      "Epoch [831/1000], Loss: 3.5406\n",
      "Epoch [832/1000], Loss: 3.5251\n",
      "Epoch [833/1000], Loss: 3.5083\n",
      "Epoch [834/1000], Loss: 3.4928\n",
      "Epoch [835/1000], Loss: 3.4779\n",
      "Epoch [836/1000], Loss: 3.4615\n",
      "Epoch [837/1000], Loss: 3.4460\n",
      "Epoch [838/1000], Loss: 3.4303\n",
      "Epoch [839/1000], Loss: 3.4147\n",
      "Epoch [840/1000], Loss: 3.3996\n",
      "Epoch [841/1000], Loss: 3.3837\n",
      "Epoch [842/1000], Loss: 3.3689\n",
      "Epoch [843/1000], Loss: 3.3536\n",
      "Epoch [844/1000], Loss: 3.3380\n",
      "Epoch [845/1000], Loss: 3.3232\n",
      "Epoch [846/1000], Loss: 3.3083\n",
      "Epoch [847/1000], Loss: 3.2938\n",
      "Epoch [848/1000], Loss: 3.2795\n",
      "Epoch [849/1000], Loss: 3.2649\n",
      "Epoch [850/1000], Loss: 3.2504\n",
      "Epoch [851/1000], Loss: 3.2359\n",
      "Epoch [852/1000], Loss: 3.2215\n",
      "Epoch [853/1000], Loss: 3.2076\n",
      "Epoch [854/1000], Loss: 3.1927\n",
      "Epoch [855/1000], Loss: 3.1787\n",
      "Epoch [856/1000], Loss: 3.1644\n",
      "Epoch [857/1000], Loss: 3.1498\n",
      "Epoch [858/1000], Loss: 3.1365\n",
      "Epoch [859/1000], Loss: 3.1220\n",
      "Epoch [860/1000], Loss: 3.1078\n",
      "Epoch [861/1000], Loss: 3.0944\n",
      "Epoch [862/1000], Loss: 3.0815\n",
      "Epoch [863/1000], Loss: 3.0685\n",
      "Epoch [864/1000], Loss: 3.0547\n",
      "Epoch [865/1000], Loss: 3.0420\n",
      "Epoch [866/1000], Loss: 3.0285\n",
      "Epoch [867/1000], Loss: 3.0154\n",
      "Epoch [868/1000], Loss: 3.0025\n",
      "Epoch [869/1000], Loss: 2.9897\n",
      "Epoch [870/1000], Loss: 2.9769\n",
      "Epoch [871/1000], Loss: 2.9641\n",
      "Epoch [872/1000], Loss: 2.9514\n",
      "Epoch [873/1000], Loss: 2.9388\n",
      "Epoch [874/1000], Loss: 2.9267\n",
      "Epoch [875/1000], Loss: 2.9136\n",
      "Epoch [876/1000], Loss: 2.9013\n",
      "Epoch [877/1000], Loss: 2.8890\n",
      "Epoch [878/1000], Loss: 2.8768\n",
      "Epoch [879/1000], Loss: 2.8647\n",
      "Epoch [880/1000], Loss: 2.8526\n",
      "Epoch [881/1000], Loss: 2.8404\n",
      "Epoch [882/1000], Loss: 2.8284\n",
      "Epoch [883/1000], Loss: 2.8163\n",
      "Epoch [884/1000], Loss: 2.8044\n",
      "Epoch [885/1000], Loss: 2.7925\n",
      "Epoch [886/1000], Loss: 2.7807\n",
      "Epoch [887/1000], Loss: 2.7688\n",
      "Epoch [888/1000], Loss: 2.7573\n",
      "Epoch [889/1000], Loss: 2.7454\n",
      "Epoch [890/1000], Loss: 2.7338\n",
      "Epoch [891/1000], Loss: 2.7222\n",
      "Epoch [892/1000], Loss: 2.7106\n",
      "Epoch [893/1000], Loss: 2.6992\n",
      "Epoch [894/1000], Loss: 2.6878\n",
      "Epoch [895/1000], Loss: 2.6763\n",
      "Epoch [896/1000], Loss: 2.6647\n",
      "Epoch [897/1000], Loss: 2.6530\n",
      "Epoch [898/1000], Loss: 2.6412\n",
      "Epoch [899/1000], Loss: 2.6294\n",
      "Epoch [900/1000], Loss: 2.6176\n",
      "Epoch [901/1000], Loss: 2.6059\n",
      "Epoch [902/1000], Loss: 2.5943\n",
      "Epoch [903/1000], Loss: 2.5827\n",
      "Epoch [904/1000], Loss: 2.5709\n",
      "Epoch [905/1000], Loss: 2.5591\n",
      "Epoch [906/1000], Loss: 2.5475\n",
      "Epoch [907/1000], Loss: 2.5359\n",
      "Epoch [908/1000], Loss: 2.5242\n",
      "Epoch [909/1000], Loss: 2.5125\n",
      "Epoch [910/1000], Loss: 2.5009\n",
      "Epoch [911/1000], Loss: 2.4894\n",
      "Epoch [912/1000], Loss: 2.4776\n",
      "Epoch [913/1000], Loss: 2.4658\n",
      "Epoch [914/1000], Loss: 2.4541\n",
      "Epoch [915/1000], Loss: 2.4431\n",
      "Epoch [916/1000], Loss: 2.4309\n",
      "Epoch [917/1000], Loss: 2.4193\n",
      "Epoch [918/1000], Loss: 2.4077\n",
      "Epoch [919/1000], Loss: 2.3962\n",
      "Epoch [920/1000], Loss: 2.3847\n",
      "Epoch [921/1000], Loss: 2.3735\n",
      "Epoch [922/1000], Loss: 2.3619\n",
      "Epoch [923/1000], Loss: 2.3508\n",
      "Epoch [924/1000], Loss: 2.3396\n",
      "Epoch [925/1000], Loss: 2.3283\n",
      "Epoch [926/1000], Loss: 2.3179\n",
      "Epoch [927/1000], Loss: 2.3066\n",
      "Epoch [928/1000], Loss: 2.2956\n",
      "Epoch [929/1000], Loss: 2.2850\n",
      "Epoch [930/1000], Loss: 2.2744\n",
      "Epoch [931/1000], Loss: 2.2633\n",
      "Epoch [932/1000], Loss: 2.2525\n",
      "Epoch [933/1000], Loss: 2.2422\n",
      "Epoch [934/1000], Loss: 2.2313\n",
      "Epoch [935/1000], Loss: 2.2210\n",
      "Epoch [936/1000], Loss: 2.2107\n",
      "Epoch [937/1000], Loss: 2.2000\n",
      "Epoch [938/1000], Loss: 2.1899\n",
      "Epoch [939/1000], Loss: 2.1802\n",
      "Epoch [940/1000], Loss: 2.1705\n",
      "Epoch [941/1000], Loss: 2.1610\n",
      "Epoch [942/1000], Loss: 2.1515\n",
      "Epoch [943/1000], Loss: 2.1419\n",
      "Epoch [944/1000], Loss: 2.1325\n",
      "Epoch [945/1000], Loss: 2.1234\n",
      "Epoch [946/1000], Loss: 2.1142\n",
      "Epoch [947/1000], Loss: 2.1051\n",
      "Epoch [948/1000], Loss: 2.0959\n",
      "Epoch [949/1000], Loss: 2.0875\n",
      "Epoch [950/1000], Loss: 2.0782\n",
      "Epoch [951/1000], Loss: 2.0696\n",
      "Epoch [952/1000], Loss: 2.0610\n",
      "Epoch [953/1000], Loss: 2.0524\n",
      "Epoch [954/1000], Loss: 2.0438\n",
      "Epoch [955/1000], Loss: 2.0357\n",
      "Epoch [956/1000], Loss: 2.0269\n",
      "Epoch [957/1000], Loss: 2.0187\n",
      "Epoch [958/1000], Loss: 2.0103\n",
      "Epoch [959/1000], Loss: 2.0023\n",
      "Epoch [960/1000], Loss: 1.9940\n",
      "Epoch [961/1000], Loss: 1.9858\n",
      "Epoch [962/1000], Loss: 1.9777\n",
      "Epoch [963/1000], Loss: 1.9697\n",
      "Epoch [964/1000], Loss: 1.9620\n",
      "Epoch [965/1000], Loss: 1.9545\n",
      "Epoch [966/1000], Loss: 1.9466\n",
      "Epoch [967/1000], Loss: 1.9388\n",
      "Epoch [968/1000], Loss: 1.9313\n",
      "Epoch [969/1000], Loss: 1.9238\n",
      "Epoch [970/1000], Loss: 1.9161\n",
      "Epoch [971/1000], Loss: 1.9088\n",
      "Epoch [972/1000], Loss: 1.9018\n",
      "Epoch [973/1000], Loss: 1.8941\n",
      "Epoch [974/1000], Loss: 1.8866\n",
      "Epoch [975/1000], Loss: 1.8798\n",
      "Epoch [976/1000], Loss: 1.8730\n",
      "Epoch [977/1000], Loss: 1.8654\n",
      "Epoch [978/1000], Loss: 1.8589\n",
      "Epoch [979/1000], Loss: 1.8519\n",
      "Epoch [980/1000], Loss: 1.8444\n",
      "Epoch [981/1000], Loss: 1.8374\n",
      "Epoch [982/1000], Loss: 1.8307\n",
      "Epoch [983/1000], Loss: 1.8235\n",
      "Epoch [984/1000], Loss: 1.8165\n",
      "Epoch [985/1000], Loss: 1.8101\n",
      "Epoch [986/1000], Loss: 1.8036\n",
      "Epoch [987/1000], Loss: 1.7970\n",
      "Epoch [988/1000], Loss: 1.7904\n",
      "Epoch [989/1000], Loss: 1.7841\n",
      "Epoch [990/1000], Loss: 1.7778\n",
      "Epoch [991/1000], Loss: 1.7714\n",
      "Epoch [992/1000], Loss: 1.7651\n",
      "Epoch [993/1000], Loss: 1.7589\n",
      "Epoch [994/1000], Loss: 1.7527\n",
      "Epoch [995/1000], Loss: 1.7465\n",
      "Epoch [996/1000], Loss: 1.7404\n",
      "Epoch [997/1000], Loss: 1.7343\n",
      "Epoch [998/1000], Loss: 1.7282\n",
      "Epoch [999/1000], Loss: 1.7222\n",
      "Epoch [1000/1000], Loss: 1.7163\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    # 数据转换\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y_area).to(torch.float)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24.0134], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([4,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "columns_name = ['x'+str(i) for i in range(9)]+['y']\n",
    "tic_data = pd.read_csv('tic_record.txt',names = columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0  x1  x2  x3  x4  x5  x6  x7  x8  y\n",
       "0   0   0   0   0   0   0   0   0   0  5\n",
       "1   0   0   0   0   0  -1   0   0   0  4\n",
       "2   0   0   0   0  -1   1   0   0   0  2\n",
       "3   0   0  -1   0   1  -1   0   0   0  8\n",
       "4   0   0   1   0  -1   1   0   0  -1  0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tic_data.iloc[:,:9].values\n",
    "y = tic_data.iloc[:,9].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 9\n",
    "hidden_size = 20\n",
    "output_size = 9\n",
    "n_iter = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.activate = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activate(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/1000], Loss: 1.4723\n",
      "Accuracy of the network : 52.656213433185115 %\n",
      "Epoch [101/1000], Loss: 1.0481\n",
      "Accuracy of the network : 64.94266323426164 %\n",
      "Epoch [151/1000], Loss: 0.8825\n",
      "Accuracy of the network : 68.59349403229581 %\n",
      "Epoch [201/1000], Loss: 0.7971\n",
      "Accuracy of the network : 70.88696466183009 %\n",
      "Epoch [251/1000], Loss: 0.7512\n",
      "Accuracy of the network : 72.03369997659723 %\n",
      "Epoch [301/1000], Loss: 0.7231\n",
      "Accuracy of the network : 72.73578282237304 %\n",
      "Epoch [351/1000], Loss: 0.7011\n",
      "Accuracy of the network : 73.25064357594196 %\n",
      "Epoch [401/1000], Loss: 0.6840\n",
      "Accuracy of the network : 73.43786566814885 %\n",
      "Epoch [451/1000], Loss: 0.6717\n",
      "Accuracy of the network : 73.74210156798502 %\n",
      "Epoch [501/1000], Loss: 0.6623\n",
      "Accuracy of the network : 73.78890709103675 %\n",
      "Epoch [551/1000], Loss: 0.6541\n",
      "Accuracy of the network : 74.18675403697637 %\n",
      "Epoch [601/1000], Loss: 0.6465\n",
      "Accuracy of the network : 74.46758717528668 %\n",
      "Epoch [651/1000], Loss: 0.6398\n",
      "Accuracy of the network : 74.74842031359701 %\n",
      "Epoch [701/1000], Loss: 0.6337\n",
      "Accuracy of the network : 74.81862859817458 %\n",
      "Epoch [751/1000], Loss: 0.6287\n",
      "Accuracy of the network : 74.79522583664873 %\n",
      "Epoch [801/1000], Loss: 0.6247\n",
      "Accuracy of the network : 74.72501755207115 %\n",
      "Epoch [851/1000], Loss: 0.6214\n",
      "Accuracy of the network : 75.02925345190732 %\n",
      "Epoch [901/1000], Loss: 0.6183\n",
      "Accuracy of the network : 75.28668382869179 %\n",
      "Epoch [951/1000], Loss: 0.6157\n",
      "Accuracy of the network : 75.38029487479523 %\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y).to(torch.long)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 验证效果\n",
    "    if (epoch>0 and epoch%50==0):\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == targets).sum().item()\n",
    "            total = targets.size(0)\n",
    "        print('Accuracy of the network : {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "currentBoard = np.array([[0,0,0,0,0,0,0,0,0]])\n",
    "input = torch.from_numpy(currentBoard).to(torch.float)\n",
    "output = model(input)\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('game')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "582c0b5fdb7e7ad1f1050fd1e5a23890c962b6c6199ffd519a966d6976476f17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
